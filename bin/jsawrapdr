#!/usr/bin/env starperl

=head1 NAME

jsawrap - process supplied files through the DR in a manner compatible with CADC

=head1 SYNOPSIS

    jsawrapdr --inputs=FILENAME --parameters="oracdr parameters" -persist
              --id="<instance id>"

=head1 DESCRIPTION

This program can call the pipeline processing in a CADC-compatible way. It
can either be run from the CADC data processing environment or from the
command line.

The program does the following:

 o calls dpRetrieve with the --inputs argument to retrieve the requested
   data from the appropriate location. Note that the non-CADC emulation
   of dpRetrieve will be able to handle files with full path as well
   as URIs.

 o looks at the data to determine the instrument and whether the data
   are raw or processed.

 o Sets up the pipeline environment variables

 o calls ORAC-DR or PICARD as appropriate with the supplied parameters.

 o Analyzes the pipeline products to determine suitable products and
   converts them to FITS format using the approved naming convention.

 o Calls dpCapture to allow products to be ingested.

=head1 ARGUMENTS

=over 4

=item B<--id>

Unique execution ID generated by external user. In the real system this
will be obtained from the master database table. Its value is passed
to the dpRetrieve and dpCapture executables.

=item B<--inputs>

Path to an input file containing all the files to be processed by
this instance of the pipeline. The file is passed directly to the
dpRetrieve executable and the contents must match that expected
by dpRetrieve. dpRetrieve ensures that the files are all available
in a local working directory.

=item B<--drcommand>

Custom data processing command to be run (instead of automatic selection
of ORAC-DR or PICARD).  The command will be run with the following
options:

=over 4

=item I<--id> Execution ID.

=item I<--inputs> File containing a listing of retrieved input data files.

=item I<--fileversion> File version number (if specified).

=item I<--transdir> Transfer directory (for final products,
      if specified and persist is selected).

=item Any specified DR parameters.

=back

=item B<--drparameters>

These are the parameters that should be passed either to ORAC-DR or to
PICARD. They should be limited to a recipe name. The wrapper will determine
whether the data should be processed by PICARD or ORAC-DR. The recipe is
optional for ORAC-DR and mandatory for PICARD.

=item B<--parameters>

This is a string containing two possible parameters, mode and
drparameters. This can be passed to jsawrapdr via CADC's data
processing recipe system, whereby these two parameters are supplied in
a database table and then passed to this script. If either 'mode' or
'drparameters' are supplied in this string, then they will override
those specific parameters as passed via --mode or --drparameters to
this script.

=item B<-persist>

Controls whether dpCapture should store the output files (if the --persist
option is given) or simply check them for compliance (if the option is not
given).

=item B<-log>

Similar to ORAC-DR C<-log> option. By default log messages are written to an
HTML log file and standard output.

=item B<-debug>

Enable debug messages.

=item B<-debugxfer>

Enable debugging directory listings for capture.

=item B<-indir>

Controls the input directory. The current working directory is used if
this argument is not supplied. Used primarily in conjunction with
C<-instrument> and C<-ut> options.

=item B<-instrument>

Specify an instrument for which data will be reduced. Must be used in
conjunction with the C<-ut> option.

=item B<-mode>

Specify the grouping mode. This does not change grouping behaviour; it
only changes collection IDs as stored by CADC. Can be one of 'night'
(for night grouping), 'project' (for project grouping) or 'public'.
If 'obs' is used group files will not be converted to FITS for
ingestion.

=item B<-outdir>

Controls the output directory for all files created by the wrapper. The current
working directory is used if this argument is not supplied. Not used if -tmpdir
is used.

=item B<-tmpdir>

Use a temporary directory within the current directory for data processing.
Supercedes -outdir.

=item B<-transdir>

Specify the transfer directory that output files will be placed
in. Supercedes -outdir.

=item B<-ut>

Reduce data for a given UT date. Skips call to dpRetrieve. Must be
used in conjunction with the C<-instrument> option.

=item B<-cleanup=option>

Mode to use when cleaning up files after execution. Options are:

  none - no cleanup performed
  cadc - cleanup all non-CADC products before calling dpCapture
  all  - cleanup all files on exit. No cleanup before dpCapture

The main difference between cadc and all is that "cadc" triggers two
cleanups. Default is "cadc".

=item B<--fileversion>

File version number.  This currently only applies to files with the
"public" grouping mode.

=back

=head1 QUESTIONS

These questions relate mainly to non-CADC usage:

- If a file lacks a PRODUCT and is an NDF it is currently assumed to be raw.
  Should the wrapper check filename convention? Should it check provenance?

- If a FITS file does not match the CADC naming convention should it still
  be processed by PICARD? Currently they are skipped.

- If an NDF does have a PRODUCT header but non-standard filename is that okay?
  Note that dpCapture only looks for standard filenames and the ndf2fits
  routine will not process unrecognized filenames.


=cut

use strict;
use warnings;

use Getopt::Long qw/GetOptionsFromString/;
use IO::File;
use IO::Uncompress::Gunzip qw/gunzip $GunzipError/;
use Pod::Usage;
use File::Copy;
use File::Spec;
use File::Temp;
use Carp;
use DateTime;
use Data::Dumper;
use Astro::FITS::Header;
use Astro::FITS::Header::NDF;
use Astro::FITS::Header::CFITSIO;
use Astro::FITS::HdrTrans;

use JSA::Starlink qw/check_star_env/;
use JSA::Convert qw/convert_to_ndf convert_dr_files/;
use JSA::Files qw/compare_file_lists scan_dir looks_like_cadcfile/;
use JSA::Headers qw/get_header_value read_headers/;
use JSA::Logging qw/log_message log_warning/;
use JSA::WrapDR qw/prepare_environment
                   retrieve_data determine_instrument
                   run_pipeline capture_products
                   clean_directory_final clean_directory_pre_capture
                   log_listing/;

print "Debugging information:\n*** ARGV:\n";
print Dumper \@ARGV;
print "*** ENV:\n";
print Dumper \%ENV;

# Options
my ($cleanup, $debug, $drcommand, $drparameters, $help, $id, $indir, $inputs,
    $instrument, $log, $man, $mode, $specified_outdir, $parameters, $persist,
    $tmpdir, $transdir, $ut, $show_version, $version, $debugxfer);

my $status = GetOptions(
    "cleanup=s"     => \$cleanup,
    "debug"         => \$debug,
    'drcommand=s'   => \$drcommand,
    "drparameters=s"=> \$drparameters,
    "help"          => \$help,
    "id=s"          => \$id,
    "indir=s"       => \$indir,
    "inputs=s"      => \$inputs,
    "instrument=s"  => \$instrument,
    "log=s"         => \$log,
    "man"           => \$man,
    "mode=s"        => \$mode,
    "outdir=s"      => \$specified_outdir,
    "parameters=s"  => \$parameters,
    "persist"       => \$persist,
    "tmpdir"        => \$tmpdir,
    "transdir=s"    => \$transdir,
    "ut=s"          => \$ut,
    "version"       => \$show_version,
    'debugxfer'     => \$debugxfer,
    'fileversion:i' => \$version,
);

pod2usage(1) if $help;
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

if ($show_version) {
    print "jsawrapdr - CADC compliant wrapper for JSA data processing\n";
    exit;
}

# Handle the "parameters" option.
if (defined($parameters)) {
  # Strip off any leading or trailing double quotes.
  $parameters =~ s/^\"//;
  $parameters =~ s/\"$//;

  $status = GetOptionsFromString($parameters,
                                 "mode=s" => \$mode,
                                 "drparameters=s" => \$drparameters);

  print "Parsed --parameters to give:\n";
  print "mode = $mode\n";
  print "drparameters = $drparameters\n";
}

# Check the UT and instrument parameters. If one's defined but not the
# other, then die with an error.
if (defined $ut && ! defined $instrument) {
    die "Must include -instrument paramter with -ut";
}
if (! defined $ut && defined $instrument) {
    die "Must include -ut parameter with -instrument";
}
my $skipdp = 0;
if (defined $ut && defined $instrument) {
    $skipdp = 1;
}

# Die if we don't have the mode, unless running a custom DR command.
unless (defined $mode) {
    die "Must supply -mode parameter to jsawrapdr"
        unless defined $drcommand;
}
else {
    # Trim any leading or trailing single quotes from mode, which might
    # possibly creep in from database-based submissions.
    $mode =~ s/^\'//;
    $mode =~ s/\'$//;
}

$cleanup = "cadc" unless defined $cleanup;

prepare_environment();

# We require that STARLINK_DIR and ORAC_DIR are set
check_star_env( "ORAC" );

# Check recipe parameters and strip any leading or trailing
# whitespace.
if (defined $drparameters) {
    $drparameters =~ s/^\s+//;
    $drparameters =~ s/\s+$//;
}

# Get the current working directory
my $curdir = File::Spec->rel2abs(File::Spec->curdir);

print "Environment variables:\n";
for my $env_variable (sort qw/PERL5LIB STARLINK_DIR SMURF_THREADS ORAC_DIR
                              PATH ADAM_USER TMPDIR HDS_SCRATCH/) {
    print "$env_variable: "
        . (defined($ENV{$env_variable}) ? $ENV{$env_variable}
                                        : "undef") . "\n";
}

# Make sure that inputs is corrected for -outdir
# but we only fiddle with $inputs if we are not running
# in grid engine. In grid engine dpRetrieve makes many assumptions
# and does not want any path to be included
if (defined $inputs) {
    $inputs = File::Spec->catfile($curdir, $inputs)
        if ! File::Spec->file_name_is_absolute($inputs);
}
elsif (! $skipdp) {
    die "Must supply a -inputs file";
}

# Change to the output directory so that dpRetrieve will copy to this
# directory regardless of dpRetrieve not having a -outdir option at CADC

my $outdir;
if ($tmpdir) {
    $outdir = File::Temp->newdir (DIR => $curdir);
    log_message("Using temporary output directory '$outdir'\n");
}
elsif (defined $specified_outdir) {
    $outdir = File::Spec->rel2abs($specified_outdir);
    log_message( "Using specified output directory '$outdir'\n");
}
else {
    $outdir = $curdir;
}
if ($outdir ne $curdir) {
    chdir($outdir) || die "Could not change directory to '$outdir'";
}

# Default the ID string since most people will not be using one
$id = "NONE" unless defined $id;

# scan the directory for plausible looking data files.
# The problem is that dpRetrieve is used to retrieve the specified data files.
# Since we do not know which files dpRetrieve retrieved we have two choices:
# 1. Die if there are any .sdf or .fits files in the directory
# 2. Scan the directory and only process files that are new (or at least
#    newer than those before calling dpRetrieve).
#
# #2 is less annoying for people but means that dpRetrieve will have to remove
# and recreate soft links if it uses soft links or use touch if the file is
# already real. This is more work to implement but for the non-JSA user
# it will be worth it.
#
# Only do this if we haven't been told to skip the dpRetrieve. Instead
# we'll supply the UT date and instrument to ORAC-DR.

my $useoracdr = 0;
my $oracinst;
my $tmpfile = '';
my @files;
my @files_retrieved = ();
my %post_retrieve;

# We need to scan for all files in the directory since dpCapture
# really does not want anything there in CADC mode
my %all_existing_files = scan_dir(qr/.*/);

unless ($skipdp) {
    # Get a hash of the existing files so that we can see which ones
    # dpRetrieve has fetched.
    my %existing_files = scan_dir();

    retrieve_data($inputs, 1);

    # Check for gzipped SDF files.
    my %gzipped_sdf = scan_dir(qr/\.sdf\.gz$/);
    foreach my $gz (compare_file_lists(\%all_existing_files, \%gzipped_sdf)) {
        log_message('Ungzipping file: ' . $gz);
        gunzip $gz => substr($gz, 0, -3) or die $GunzipError;
    }

    # Now scan the directory again
    %post_retrieve = scan_dir();

    # compare the old list with the new and return a list of files that
    # should be processed
    @files = compare_file_lists( \%existing_files, \%post_retrieve );

    # Check we got the correct number of files.
    do {
        my $fh = new IO::File($inputs);
        my $n = 0;
        while (<$fh>) {
              $n ++ if /\w/ && ! /^\#/;
        }
        $fh->close();
        die 'Incorrect number of files retrieved: expected ' .
            $n . ' but got ' . (scalar @files)
            unless $n == (scalar @files);
    };

    # Keep a copy of the list of files which were retrieved so that they
    # can be cleaned up before running dpCapture.
    @files_retrieved = @files;

    log_message("Processing the following files:\n". join("\n",@files)."\n");

    # First thing we have to do is to analyze the files to determine how
    # things should be processed. If we are all FITS files then we must
    # convert to NDF prior to processing. If there are some FITS and
    # some NDF we have to decide whether that is okay. Read all the FITS
    # headers and then decide whether they are all raw or all processed.

    # Now count NDF vs FITS
    my $nfits = 0;
    my $nsdf = 0;
    for my $f (@files) {
        if ($f =~ /\.f\w+$/) {
            $nfits++;
        }
        elsif ($f =~ /\.sdf$/) {
            $nsdf++;
        }
    }

    if (($nfits != @files) && ($nsdf != @files)) {
        # mixed NDF and FITS
        die "There are $nfits FITS files and $nsdf NDFs out of ".@files.
            " files and this is a bit confusing";
    }

    unless (defined $drcommand) {
        # Decide on ORAC-DR vs PICARD

        # FITS headers, indexed by filename
        my %FITS = read_headers( @files );
        if (keys %FITS == 0) {
            die "Could not open any of the supplied files to read headers: " .
                join(", ",@files);
        }
        elsif (keys %FITS != @files) {
            die "Some files could not be read: " .
                join(', ', grep {not exists $FITS{$_}} @files);
        }

        # FITS files are not processable by ORAC-DR and are not archived as
        # such at CADC NDF are processed by ORAC-DR unless they have PRODUCT
        # headers. No check is made for filename compliance with raw time
        # series ICD.

        # Get PRODUCT information
        my ($nproduct, %products) = get_header_value("PRODUCT", values %FITS);

        $useoracdr = 0;
        if ($nfits == @files) {
            # All of them are FITS so we must use PICARD
            log_message("Got $nfits fits - using PICARD\n");
        }
        elsif ($nsdf == @files) {
            # we have NDFs so we need to open them and look for PRODUCT
            # headers Should we check to make sure that the PRODUCT headers
            # match?
            log_message("Got $nsdf NDF\n");
            if ($nproduct == 0) {
                # oracdr - assume these are raw data. They may be data processed
                # outside of the DR
                $useoracdr = 1;
            }
        }

        # Analyze product information for non-ORAC-DR case
        if (!$useoracdr) {
            if ($nproduct == @files) {
                # check product values
                if (keys %products > 1) {
                    log_warning("Processing different products in a single run may be " .
                                "incorrect. (".  join(",",keys %products).")\n");
                }
            }
            elsif ($nproduct == 0) {
                die "None of the input files contained PRODUCT FITS headers";

            }
            else {
                die "There are $nproduct headers in ".@files.
                  ", making it difficult to determine whether to run ORAC-DR or PICARD";
            }
        }

        if ($useoracdr) {
            $oracinst = determine_instrument(\%FITS);
        }
    }

    # Convert FITS files into NDF and update name supplied to DR. We
    # could do this before we read the FITS headers but it is more
    # efficient to delay conversion in case some other error condition
    # is hit first. Conversion will take a non-trivial amount of time

    # For now assume that we want to convert to NDF even if using a
    # custom-specified DR command.
    if ($nfits > 0) {
        my @new;
        for my $f (@files) {
            if (looks_like_cadcfile( $f )) {
                my $out = convert_to_ndf( $f );
                if ($out) {
                    push(@new, $out );
                }
            } else {
                log_warning("File $f does not use the CADC naming convention. " .
                            "Skipping.\n");
            }
        }
        if (@new) {
            @files = @new;

            # Scan the directory again, so that it includes the converted files
            # and we don't mistake them for new pipeline products.
            %post_retrieve = scan_dir();
        }
        else {
            die "No files left to process after conversion to NDF";
        }
    }
}
else {
    %post_retrieve = scan_dir();
    $useoracdr = 1;
    $oracinst = $instrument;
}

# Pass additional information to run_pipeline (e.g. id, version, ...)
# in case we are running a custom DR command.  It may need this information
# to convert and store its output files.
run_pipeline($useoracdr, $oracinst, $indir, $outdir,
             ($skipdp ? "$ut" : \@files), $drparameters,
             {batch => 1, drcommand => $drcommand,
              id => $id, version => $version,
              persist => $persist, transdir => $transdir});

# Assume that the custom command has converted and stored its outputs,
# since we don't know what its naming conventions, etc. might be.
unless (defined $drcommand) {
    # Process NDF data products and convert them into FITS
    #  - Need to filter provenance
    #  - Run NDF2FITS

    # scan the directory again
    my %post_process = scan_dir();
    my @drfiles = compare_file_lists(\%post_retrieve, \%post_process);

    # Read all the FITS headers from these files
    my %drhdrs = read_headers(@drfiles);

    # And convert those files that have valid PRODUCT headers.
    convert_dr_files(\%drhdrs, {"mode" => $mode, dpid => $id,
                                dpdate => DateTime->now->datetime,
                                version => $version});

    # TEMPORARY HACK: copy all log.* files into the ORAC_LOGDIR if defined.
    if (exists $ENV{'ORAC_LOGDIR'}) {
        my $output_dir = $ENV{'ORAC_LOGDIR'};
        my %logfiles = scan_dir(qr/log\./);
        foreach my $f (keys %logfiles) {
            copy($f, $output_dir) or die "copy from $f to $output_dir failed: $!";
        }
    }

    log_listing('pre-cleanup', '.') if $debugxfer;

    if ($cleanup eq 'cadc') {
        clean_directory_pre_capture($outdir, \%all_existing_files,
                                    \&looks_like_cadcfile);

        # If working in a temporary directory (e.g. at CADC) or a
        # specified output directory (e.g. in the jsa_proc system), make sure
        # all the retrieved files have been deleted.  If we are working
        # on processed data with PICARD then we don't want to send back
        # the original files.
        unlink grep {-e $_} map {File::Spec->catfile($outdir, $_)} @files_retrieved
            if ($tmpdir or $specified_outdir);
    }

    log_listing('post-cleanup', '.') if $debugxfer;

    capture_products($persist, $transdir, 0);
}

log_listing('transdir', $transdir) if $transdir && $debugxfer;

clean_directory_final($outdir, \%all_existing_files)
    unless $cleanup eq 'none';

exit;

=head1 NOTES

ORAC-DR will be run such that the following options will be enabled
automatically:

 -nodisplay
 -log hs
 -file
 -loop file
 -batch

The handling of calibration options is still TBD.

PICARD is run with 

 -log hs

The output from the pipeline processes is harvested automatically.

The choice of ORAC-DR or PICARD can be made by looking at the input data.
If the first file in the input file includes a PRODUCT header then it must
have been processed. Additionally, if the input is in FITS format then
it must have been processed.

=head1 ENVIRONMENT

This program requires that $STARLINK_DIR and $ORAC_DIR environment variables
are correctly defined.

Exit status is non-zero if any problem occurred during the data processing.

All files are written into the current working directory by default.

=head1 AUTHORS

Tim Jenness E<lt>t.jenness@jach.hawaii.eduE<gt>,

=head1 COPYRIGHT

Copyright (C) 2008-2011 Science and Technology Facilities Council.
All Rights Reserved.

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.

This program is distributed in the hope that it will be useful,but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
this program; if not, write to the Free Software Foundation, Inc., 59 Temple
Place,Suite 330, Boston, MA  02111-1307, USA

=cut
